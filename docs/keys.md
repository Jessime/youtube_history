# Making the dataframe

I'm running into some memory usage issues since it seems that Google's outputting a lot of information. The latest run I did pulled down 10G of json, which I'm trying and failing to load into ram in a dataframe. Instead of making the giant dataframe, I'm just going to keep the columns I'm using, which is a pretty small subset. Here's some info about what columns are available and how large they are, based of one random example:


```
[(4696, 'automatic_captions'),
 (3512, 'description'),
 (1240, 'formats'),
 (1189, 'url'),
 (920, 'heatmap'),
 (640, 'subtitles'),
 (472, 'thumbnails'),
 (232, 'http_headers'),
 (232, 'downloader_options'),
 (232, '_version'),
 (184, '_format_sort_fields'),
 (107, 'thumbnail'),
 (105, 'channel_url'),
 (92, 'webpage_url'),
 (88, 'categories'),
 (81, 'uploader_url'),
 (73, 'channel_id'),
 (71, 'title'),
 (71, 'fulltitle'),
 (68, 'format'),
 (60, 'webpage_url_domain'),
 (60, 'vcodec'),
 (60, 'id'),
 (60, 'display_id'),
 (58, 'acodec'),
 (57, 'uploader_id'),
 (57, 'upload_date'),
 (57, 'live_status'),
 (56, 'uploader'),
 (56, 'tags'),
 (56, 'resolution'),
 (56, 'extractor_key'),
 (56, 'extractor'),
 (56, 'channel'),
 (55, 'availability'),
 (54, 'webpage_url_basename'),
 (54, 'protocol'),
 (54, 'duration_string'),
 (54, '_type'),
 (53, 'format_note'),
 (53, 'audio_ext'),
 (52, 'video_ext'),
 (52, 'ext'),
 (52, 'dynamic_range'),
 (51, 'language'),
 (51, 'format_id'),
 (32, 'timestamp'),
 (32, 'epoch'),
 (28, 'width'),
 (28, 'view_count'),
 (28, 'source_preference'),
 (28, 'playable_in_embed'),
 (28, 'like_count'),
 (28, 'language_preference'),
 (28, 'height'),
 (28, 'fps'),
 (28, 'filesize_approx'),
 (28, 'filesize'),
 (28, 'duration'),
 (28, 'comment_count'),
 (28, 'channel_is_verified'),
 (28, 'channel_follower_count'),
 (28, 'audio_channels'),
 (28, 'asr'),
 (24, 'was_live'),
 (24, 'tbr'),
 (24, 'quality'),
 (24, 'is_live'),
 (24, 'has_drm'),
 (24, 'aspect_ratio'),
 (24, 'age_limit')]
 ```
 